{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mourad\\Documents\\Git\\SL_data_perception\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://bouman:80um4N!@ec2-15-188-255-64.eu-west-3.compute.amazonaws.com:27017/\")\n",
    "db = client[\"bouman_datatank\"]\n",
    "collection = db[\"articles\"]\n",
    "\n",
    "text = collection[\"text\"].find().limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x2a682a86c20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pymongo.cursor.Cursor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "no such item for Cursor instance",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Corpus with example sentences\u001b[39;00m\n\u001b[0;32m      4\u001b[0m corpus \u001b[39m=\u001b[39m text\n\u001b[1;32m----> 5\u001b[0m corpus_embeddings \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39;49mencode(corpus, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Query sentences:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m queries \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWhich articles deal with the subject of data protection?\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Mourad\\Documents\\Git\\SL_data_perception\\env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m--> 161\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sentences_batch)\n\u001b[0;32m    162\u001b[0m     features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Mourad\\Documents\\Git\\SL_data_perception\\env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: Union[List[\u001b[39mstr\u001b[39m], List[Dict], List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]):\n\u001b[0;32m    316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[1;32mc:\\Users\\Mourad\\Documents\\Git\\SL_data_perception\\env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:102\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    100\u001b[0m batch1, batch2 \u001b[39m=\u001b[39m [], []\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m text_tuple \u001b[39min\u001b[39;00m texts:\n\u001b[1;32m--> 102\u001b[0m     batch1\u001b[39m.\u001b[39mappend(text_tuple[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    103\u001b[0m     batch2\u001b[39m.\u001b[39mappend(text_tuple[\u001b[39m1\u001b[39m])\n\u001b[0;32m    104\u001b[0m to_tokenize \u001b[39m=\u001b[39m [batch1, batch2]\n",
      "File \u001b[1;32mc:\\Users\\Mourad\\Documents\\Git\\SL_data_perception\\env\\lib\\site-packages\\pymongo\\cursor.py:762\u001b[0m, in \u001b[0;36mCursor.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m clone:\n\u001b[0;32m    761\u001b[0m         \u001b[39mreturn\u001b[39;00m doc\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mno such item for Cursor instance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    763\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m cannot be applied to Cursor instances\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m index)\n",
      "\u001b[1;31mIndexError\u001b[0m: no such item for Cursor instance"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = text\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\"Which articles deal with the subject of data protection?\"]\n",
    "# queriez = [\"data\", \"données\", \"data use\", \"data reusability\", \"data reuse\",\"data sharing\", \"data access\", \"data privacy\",\"data protection\",\"gdpr\",\"rgpd\", \"protection des données\", \"utilisation des données\", \"accès aux données\", \"confidentialité des données\"]\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    condition = cos_scores.abs() > 0.20\n",
    "\n",
    "    for article, result, score in zip(articles, condition, cos_scores):\n",
    "        if result == True :\n",
    "            print(score, article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
